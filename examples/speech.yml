---

###############################################################################
# Pro-tips:
#
# - Use YAML's anchors! This let's you define a value (even a dictionary), and
#   reuse, or even change, parts of it later. YAML anchors are incredible for
#   defining constant values that you want to reuse all over the place. You can
#   define an anchor like this:
#     KEY: &my_anchor
#   Note that the value of KEY can be anything. All of these are allowed:
#     KEY: &my_anchor "my value"
#     KEY: &my_ancor
#       A: B
#       C: D
#     KEY: &my_anchor [1, 2, 3]
#   You can then refer back to your anchors like this:
#     ANOTHER_KEY: *my_anchor
#   This sets the value of ANOTHER_KEY to be the same thing as the original
#   KEY. Now let's say that your anchor is a dictionary, but you want to refer
#   to it with modified values later. Try this:
#     KEY: &my_anchor
#       FIRST: VALUE_1
#       SECOND: VALUE_2
#     ANOTHER_KEY:
#       <<: *my_anchor
#       SECOND: VALUE_2_NEW
#       THIRD: VALUE_3
#     MORE_KEY: *my_anchor
#   These are 100% equivalent to this more verbose structure:
#     KEY:
#       FIRST: VALUE_1
#       SECOND: VALUE_2
#     ANOTHER_KEY:
#       FIRST: VALUE_1
#       SECOND: VALUE_2_NEW
#       THIRD: VALUE_3
#     MORE_KEY:
#       FIRST: VALUE_1
#       SECOND: VALUE_2
#
# - Use the Jinja2 engine! It is really powerful, and it's most appropriately
#   used to do on-the-fly interpretation/evaluation of values in the "model"
#   section of the Kurfile.
#
# - So how do you know when to use YAML anchors as opposed to Jinja2
#   expressions? Here are some tips.
#
#   YAML anchors only work within a single YAML file, and are evaluated the
#   moment the file is loaded. This means you can't use YAML anchors from a
#   JSON Kurfile, and you can't reference anchors in other Kurfiles.
#
#   Jinja2 is interpreted after all Kurfiles are loaded, which means that
#   many different Kurfiles can share variables via Jinja2. Jinja2
#   expressions can also be used in JSON Kurfiles.
#
#   It's almost like YAML anchors are "compile-time constants" but Jinja2
#   expressions are interpreted at run-time. As a result, the value of a
#   Jinja2 expression could be different at different points in the
#   Kurfile (e.g., if you use Jinja2 to reference the previous layer in a
#   model, obviously the interpretation/value of "previous layer" resolves
#   to something different for the second layer in the model as compared to the
#   fifth layer in the model.

###############################################################################
settings:

  # Deep learning model
  cnn:
    kernels: 1000
    size: 11
    stride: 2
  rnn:
    size: 1000
    depth: 3
  vocab:
    # Need for CTC
    size: 28

  # Setting up the backend.
  backend:
    name: keras
    backend: tensorflow

  # Batch sizes
  provider: &provider
    batch_size: 16
    force_batch_size: yes

  # Where to put the data.
  data: &data
    path: "~/kur"
    type: spec
    max_duration: 50
    max_frequency: 8000
    normalization: norm.yml

  # Where to put the weights
  weights: &weights weights

###############################################################################
model:

  # This is Baidu's DeepSpeech model:
  #   https://arxiv.org/abs/1412.5567
  # Kur makes prototyping different versions of it incredibly easy.

  # The model input is audio data (called utterances).
  - input: utterance

  # One-dimensional, variable-size convolutional layers to extract more
  # efficient representation of the data.
  - convolution:
      kernels: "{{ cnn.kernels }}"
      size: "{{ cnn.size }}"
      strides: "{{ cnn.stride }}"
      border: valid
  - activation: relu
  - batch_normalization

  # A series of recurrent layers to learn temporal sequences.
  - for:
      range: "{{ rnn.depth }}"
      iterate:
        - recurrent:
            size: "{{ rnn.size }}"
            sequence: yes
        - batch_normalization

  # A dense layer to get everything into the right output shape.
  - parallel:
      apply:
        - dense: "{{ vocab.size + 1 }}"
  - activation: softmax

  # The output is the transcription.
  - output: asr

###############################################################################
train:

  data:
    # A "speech_recognition" data supplier will create these data sources:
    #   utterance, utterance_length, transcript, transcript_length, duration
    - speech_recognition:
        <<: *data
        url: "https://kur.deepgram.com/data/lsdc-train.tar.gz"
        checksum: >-
          fc414bccf4de3964f895eaa9d0e245ea28810a94be3079b55505cf0eb1644f94

  weights: *weights
  provider:
    <<: *provider
    sortagrad: duration

  log: log

  optimizer:
    name: sgd
    nesterov: yes
    learning_rate: 2e-4
    momentum: 0.9
    clip:
      norm: 100

###############################################################################
validate: &validate
  data:
    - speech_recognition:
        <<: *data
        url: "https://kur.deepgram.com/data/lsdc-test.tar.gz"
        checksum: >-
          e1c8cf9cd57e8c1ae952b6e4e40dcb5c8e3932c81ecd52c090e4a05c8ebbea2b

  weights: *weights
  provider: *provider

  hooks:
    - transcript

###############################################################################
test: *validate

###############################################################################
evaluate: *validate

###############################################################################
loss:
  - name: ctc
    # The model's output (its best-guest transcript).
    target: asr
    # How long the corresponding audio utterance is.
    input_length: utterance_length
    relative_to: utterance
    # How long the ground-truth transcript is.
    output_length: transcript_length
    # The ground-truth transcipt itself.
    output: transcript

...
