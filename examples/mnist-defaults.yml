---
# Declare the loss function that is used during training/validation/testing.
loss:
  # It is a list of loss functions, one for each model output.
  # The MNIST example only has one output, named "labels".
  - target: labels
    name: categorical_crossentropy

# The "include" section in "mnist.yml" is magical, and will merge this section
# into the section in "mnist.yml".
train:
  # Let's include checksums for all of the data we download.
  data:
    - mnist:
        images:
          checksum: 440fcabf73cc546fa21475e81ea370265605f56be210a4024d2ca8f203523609
          path: "~/kur"
        labels:
          checksum: 3552534a0a558bbed6aed32b30c495cca23d567ec52cac8be1a0730e8010255c
          path: "~/kur"

  # As we discuss in "Examples" in the documentation, we only train on the first
  # batches each epoch. This is just to make getting through the MNIST example
  # nice and quick on slow/CPU-only machines. If you have a GPU, feel free to
  # remove the "provider:" section entirely.
  provider:
    num_batches: 1
  log: ../../kur_examples/mnist/mnist-log # remember if don't set log, your model won't build on previous runs
  # How many epochs to train for.
  epochs:
    number: 1
    mode: additional
  weights: # the folders below are prepared automatically?
    initial: ../../kur_examples/mnist/mnist.best.valid.w
    best: ../../kur_examples/mnist/mnist.best.train.w
    last: ../../kur_examples/mnist/mnist.last.w
  hooks:
    - plot_weights:
        plot_every_n_epochs: 1
        plot_directory: ../../kur_examples/mnist/mnist_plot_weights
        weight_file: ../../kur_examples/mnist/mnist.best.valid.w
        with_weights:
          - ["kernel", "dense"]
          - ["kernel", "convol"]
          - ["weight", "dense"]   # pytorch use weight, keras use kernel
          - ["weight", "convol"]
# Here, we use the MNIST test set as a validation set (more generally, you'll
# want train, validation, and test sets; but we ignore this for the MNIST
# example). The funky "&validation" is just a YAML anchor, so we can reference
# this section later.
validate: &validate
  data:
    - mnist:
        images:
          url: "http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz"
          checksum: 8d422c7b0a1c1c79245a5bcf07fe86e33eeafee792b84584aec276f5a2dbc4e6
          path: "~/kur"
        labels:
          url: "http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz"
          checksum: f7ae60f92e00ec6debd23a6088c31dbd2371eca3ffa0defaefb259924204aec6
          path: "~/kur"

  # Let's also use less data for validation, just to speed it along.
  provider:
    num_batches: 100

  # Where to save the model weights that have the lowest validation loss.
  weights: ../../kur_examples/mnist/mnist.best.valid.w

# Let's define the test set, used if you execute something like:
# $ kur test mnist.yml
# The funky "*validation" is just a YAML alias, so we basically are setting the
# "test" section" to be the same as the "validate" section.
test: *validate

# This is the evaluation section, used during `kur evaluate mnist.yml`.
# The funky "<<: *validate" is just YAML, and basically means "copy all of the
# keys from 'validate', and then add/replace the
evaluate:
  <<: *validate

  # Use the entire testing set for evaluation.
  provider:
    num_batches: null

  # Where do we want to store the output file?
  # Here, we are just storing it as a Python pickle.
  destination: ../../kur_examples/mnist/mnist.results.pkl

  # This is a list of post-processing hooks. Here, we want to produce the
  # digit-by-digit accuracy table (just called "mnist").
  hooks:
    - mnist
...
